{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b2d9bf-000b-4c78-826a-43644f8f7f4b",
   "metadata": {},
   "source": [
    "# Accessing CMIP6 Data via Amazon Web Services\n",
    "### Authors\n",
    "\n",
    "Samantha Stevenson sstevenson@ucsb.edu\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[Goals](#purpose)\n",
    "\n",
    "[Import Packages](#path)\n",
    "\n",
    "[Read in Data](#data_io)\n",
    "\n",
    "[Plot a Time Series](#time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b4f24-a58c-49e0-8697-0dd93268b21a",
   "metadata": {},
   "source": [
    "<a id='purpose'></a> \n",
    "## **Goals**\n",
    "\n",
    "In this tutorial, we will be using the database of Coupled Model Intercomparison Project phase 6 (CMIP6) output hosted by Amazon Web Services to create a basic time series visualization. \n",
    "\n",
    "The steps in this tutorial build on the various skills we learned in previous tutorials:\n",
    "- [Read in Data and Plot a Time Series](https://github.com/climate-datalab/Time-Series-Plots/blob/main/1.%20Read%20in%20Climate%20Data%20%2B%20Plot%20a%20Regionally%20Averaged%20Time%20Series.ipynb)\n",
    "  (regional averaging, time series plotting)\n",
    "  \n",
    "- [Mean-State and Seasonal Average Maps](https://github.com/climate-datalab/Map-Plots/blob/main/2.%20Mean-State%20and%20Seasonal%20Difference%20Plots.ipynb)\n",
    "  (concatenating xarray objects)\n",
    "\n",
    "Basically: we'll be doing a lot of the same things we did in those tutorials, but accessing data via the cloud instead of downloading the files to a local machine/server! Please refer back to those materials if you would like additional detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13965a9-d341-4bc1-a0d5-ffc43c85b570",
   "metadata": {},
   "source": [
    "<a id='path'></a> \n",
    "## **Import Packages**\n",
    "\n",
    "As always, we begin by importing the necessary packages for our analysis. The packages that are new for this tutorial are:\n",
    "- `intake` \n",
    "- `intake-esm`\n",
    "- `s3fs`\n",
    "\n",
    "The idea behind `intake` is that it can be a unified interface regardless of the data source on a remote server, which provides a consistent API regardless of where the data is or what format it's stored in. It relies on \"catalogs\" of data on the remote server, which contain inventories of all the data available and the locations in which it's stored. `intake` also interfaces really well with packages like pandas and xarray - basically, it lets you synthesize a bunch of data on a server and read it in quickly as an easy-to-manipulate object within Python.\n",
    "\n",
    "In addition to `intake`, `intake-esm` is also needed to parse the CMIP6 data catalogs we're working with today. `intake-esm` is a plugin that layers on top of `intake` - so it actually requires that `intake` be installed in order to function. `intake-esm` provides additional tools to search, filter, and load netCDF information (or, as we'll see later, \"zarr\" format data) and understands the metadata structure associated with CMIP6 and many other ensembles of climate information.\n",
    "\n",
    "The final new package we'll need is `s3fs`, which provides a file system interface to the [Amazon Simple Storage Service (S3)](https://aws.amazon.com/s3/). This allows a user to read and write files directly from the S3 server, and integrates with xarray and intake. \n",
    "\n",
    "More detail on how intake, intake-esm, and s3fs work can be found at:\n",
    "- The [intake Read the Docs page](https://intake.readthedocs.io/en/latest/scope2.html)\n",
    "- The [intake-esm Read the Docs page](https://intake-esm.readthedocs.io/en/v2021.8.17/user-guide/index.html)\n",
    "- This handy [Youtube explainer](https://www.youtube.com/watch?v=QVogieGP4Jw)\n",
    "- The [s3fs Read the Docs page](https://s3fs.readthedocs.io/en/latest/)\n",
    "\n",
    "To install these packages, you can use either pip or conda, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4df3780-3893-4811-b5eb-1e9ac562018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import intake\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d4a4a0-5b21-42b8-8194-cb36342a379c",
   "metadata": {},
   "source": [
    "<a id='data_io'></a> \n",
    "## **Read in Data** \n",
    "\n",
    "The next step is to read the data we'd like for our analysis into Python. Here we will NOT be downloading any files to a local machine! Instead, we'll rely on one of the various catalogs of climate model output hosted on cloud computing servers. This one is a set of CMIP6 output maintained by Amazon Web Services. \n",
    "\n",
    "You can find more information on the data catalog here:\n",
    "\n",
    "[Blog post: CMIP6 provided through the Amazon Sustainability Data Initiative](https://aws.amazon.com/blogs/publicsector/now-available-cmip6-dataset-foster-climate-innovation-study-impact-future-climate-conditions/)\n",
    "\n",
    "[Registry of Open Data (AWS)](https://registry.opendata.aws/cmip6/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfc3c0-547e-48a8-9f7e-c9d3b3194f48",
   "metadata": {},
   "source": [
    "### **Use intake to open data catalog**\n",
    "\n",
    "Let's first take a look at the whole data catalog to get a sense of what's in there! \n",
    "\n",
    "The `intake-esm` package contains a function called `open_esm_datastore` which can read the JSON file describing the contents of the CMIP6 data holdings. This will be parsed and can be stored as a \"catalog\" object that can be further queried within Python to grab the part of it a user is interested in.\n",
    "\n",
    "More details on `open_esm_datastore`:\n",
    "\n",
    "[Read the Docs \"Loading a Catalog\" page](https://intake-esm.readthedocs.io/en/v2021.8.17/user-guide/overview.html#loading-a-catalog)\n",
    "\n",
    "(note that the link above uses a different data catalog than the one we're working with here, but the principle is the same!)\n",
    "\n",
    "_**The CMIP6 data catalog is quite large, so this code block may take a minute to run:**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eafab4e7-bbe3-4368-a8db-b78b50d34f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CMIP6 data catalog, store as a variable\n",
    "catalog = intake.open_esm_datastore('https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cb0e1e-d2e0-478a-a64a-5332d79441d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>pangeo-cmip6 catalog with 7780 dataset(s) from 522217 asset(s)</strong>:</p> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>activity_id</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>institution_id</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_id</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment_id</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member_id</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table_id</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable_id</th>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grid_label</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zstore</th>\n",
       "      <td>522217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcpp_init_year</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>version</th>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the catalog to get a summary of its contents\n",
    "catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d0443-eb59-4f67-a76d-03f81f83a1be",
   "metadata": {},
   "source": [
    "### **Search the catalog for a specific dataset**\n",
    "\n",
    "As you can see from the code block above, there is an enormous amount of data in this catalog! We definitely don't want to look at the entire thing all at once.\n",
    "\n",
    "Let's do an example of pulling the data we used in previous tutorials, this time from the cloud. The example data file from the [Time Series Plots](https://github.com/climate-datalab/Time-Series-Plots) and [Map Plots](https://github.com/climate-datalab/Map-Plots) repositories is:\n",
    "\n",
    "`tas_Amon_CanESM5_historical_r10i1p1f1_gn_185001-201412.nc`\n",
    "\n",
    "We can break this down to extract the fields we'll need to search the data catalog properly. If you need more detail on how to do this, also refer to the [filename decoder](http://climate-datalab.org/filename-decoder/) on the Climate DataLab website!\n",
    "\n",
    "#### **Characteristics of this file**:\n",
    "- _Variable_: This is a surface air temperature, or \"tas\", variable.\n",
    "- _Realm_: Surface air temperature is generated by the atmosphere component of a climate model (\"A\"), and the information in this particular file is averaged monthly (\"mon\").\n",
    "- _Model_: The name of the model is \"CanESM5\", which is short for the Canadian Earth System Model version 5.\n",
    "- _Ensemble member_: The name of this ensemble member is \"r10i1p1f1\".\n",
    "- _Grid_: This output is provided on the model's _native grid_ (\"gn\"), instead of doing any kind of interpolating to a different grid.\n",
    "\n",
    "#### **How to search the catalog for these things**:\n",
    "\n",
    "In order to find the equivalent information in the CMIP6 AWS catalog, we need to know what the appropriate search terms are. The terminology that AWS uses is slightly different from the way that things are specified on the CMIP6 website, because why make things too easy....?\n",
    "\n",
    "You can see the fields that are listed in the AWS catalog from the `print(catalog)` statement above. Here is a translation chart to explain the most important ones (the fields that you'll generally be searching over):\n",
    "\n",
    "- `activity_id`: This is the name of the \"activity\", or overall model intercomparison project (MIP), you're interested in. There are a lot of these, and you don't need to worry about most of them right now! (The idea behind the MIPs is explained in the [CMIP and other MIPs](https://climate-datalab.org/cmip-and-sub-mips/) page on the Climate DataLab website).\n",
    "\n",
    "   For most applications, the ones you'll want are `CMIP` and `ScenarioMIP`. The `CMIP` activity is where the data for the historical period (1850-2015) is located, and the `ScenarioMIP` activity contains all the future projections (2015-2100).\n",
    "  \n",
    "- `source_id`: This is the name of the actual climate model you're interested in. In our case, we want CanESM5! \n",
    "\n",
    "- `institution_id`: This is the name of the \"institution\", or modeling center, which ran a given simulation. _Don't worry too much about this one_, because you can just search by the name of the model itself and get the same result. But for reference here: the modeling center which created the CanESM5 is the Canadian Centre for Climate Modeling and Analysis. See the [Models vs Modeling Centers](https://climate-datalab.org/models-vs-modeling-centers/) explainer on the Climate DataLab site if you're curious about how this works!\n",
    "\n",
    "- `experiment_id`: This is the name of the specific type of \"experiment\" included in CMIP or ScenarioMIP. The ones you'll want here are `historical` (which is part of the CMIP \"activity\"), and one of the SSP future scenarios (which are part of ScenarioMIP). \n",
    "\n",
    "  You can pick which futures you're interested in! The main four scenarios used for CMIP6 are `ssp126`, `ssp245`, `ssp370`, and `ssp585`. Here higher numbers after `ssp` mean more overall warming (technically, the numbers are equal to the \"radiative imbalance\" at the top of the atmosphere, or difference between energy coming in and going out). \n",
    "\n",
    "- `member_id`: This is the name of the individual ensemble member run for a given \"experiment\" and \"source_id\". In our case, we're looking for a member_id of r10i1p1f1.\n",
    "\n",
    "- `table_id`: This is equivalent to the \"realm\" terminology used on the Earth System Grid; basically, which portion of the Earth do you want to be looking at? They're in different tables in the cloud database. In this case, we want the monthly averages for atmospheric variables, which is a table id of \"Amon\".\n",
    "\n",
    "- `variable_id`: This is the name of the individual variable you're interested in visualizing. In this case, we're interested in surface air temperature, or \"tas\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7329ec43-8cd9-4e4f-8a85-54a42a3c83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify search terms to query catalog for CanESM5 data\n",
    "# activity_id: which project do you want? CMIP = historical data, ScenarioMIP = future projections\n",
    "activity_ids = ['ScenarioMIP', 'CMIP'] \n",
    "\n",
    "# source_id: which model do you want? \n",
    "source_id = ['CanESM5']\n",
    "\n",
    "# experiment_id: what experimental configuration do you want? Here we want historical and the four main SSPs\n",
    "experiment_ids = ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "# member_id: which ensemble member do you want? Here we want r10i1p1f1\n",
    "member_id = 'r10i1p1f1'\n",
    "\n",
    "# table_id: which part of the Earth system and time resolution do you want? Here we want monthly atmosphere data\n",
    "table_id = 'Amon' \n",
    "\n",
    "# variable_id: which climate variable do you want? Here we want surface air temperature\n",
    "variable_id = 'tas' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658080e-5e4b-49bf-9d40-ad0ca8ada62d",
   "metadata": {},
   "source": [
    "#### **Display catalog search results**\n",
    "\n",
    "The code block above specifies the search terms to use to get the `r10i1p1f1` member of the CanESM5 historical and SSP ensembles. To actually retrieve the information, we use the `.search` functionality that `catalog` type objects possess. \n",
    "\n",
    "The code block below parses through the full CMIP6 catalog and retrieves only entries that satisfy our search criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64d7ea4-e581-46dd-8dc4-0eccc3e6109c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>zstore</th>\n",
       "      <th>dcpp_init_year</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>CCCma</td>\n",
       "      <td>CanESM5</td>\n",
       "      <td>historical</td>\n",
       "      <td>r10i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/CMIP/CCCma/CanESM5/histor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CCCma</td>\n",
       "      <td>CanESM5</td>\n",
       "      <td>ssp585</td>\n",
       "      <td>r10i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CCCma</td>\n",
       "      <td>CanESM5</td>\n",
       "      <td>ssp370</td>\n",
       "      <td>r10i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CCCma</td>\n",
       "      <td>CanESM5</td>\n",
       "      <td>ssp126</td>\n",
       "      <td>r10i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ScenarioMIP</td>\n",
       "      <td>CCCma</td>\n",
       "      <td>CanESM5</td>\n",
       "      <td>ssp245</td>\n",
       "      <td>r10i1p1f1</td>\n",
       "      <td>Amon</td>\n",
       "      <td>tas</td>\n",
       "      <td>gn</td>\n",
       "      <td>s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activity_id institution_id source_id experiment_id  member_id table_id  \\\n",
       "0         CMIP          CCCma   CanESM5    historical  r10i1p1f1     Amon   \n",
       "1  ScenarioMIP          CCCma   CanESM5        ssp585  r10i1p1f1     Amon   \n",
       "2  ScenarioMIP          CCCma   CanESM5        ssp370  r10i1p1f1     Amon   \n",
       "3  ScenarioMIP          CCCma   CanESM5        ssp126  r10i1p1f1     Amon   \n",
       "4  ScenarioMIP          CCCma   CanESM5        ssp245  r10i1p1f1     Amon   \n",
       "\n",
       "  variable_id grid_label                                             zstore  \\\n",
       "0         tas         gn  s3://cmip6-pds/CMIP6/CMIP/CCCma/CanESM5/histor...   \n",
       "1         tas         gn  s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...   \n",
       "2         tas         gn  s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...   \n",
       "3         tas         gn  s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...   \n",
       "4         tas         gn  s3://cmip6-pds/CMIP6/ScenarioMIP/CCCma/CanESM5...   \n",
       "\n",
       "   dcpp_init_year   version  \n",
       "0             NaN  20190429  \n",
       "1             NaN  20190429  \n",
       "2             NaN  20190429  \n",
       "3             NaN  20190429  \n",
       "4             NaN  20190429  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Search through catalog, store results in \"res\" variable\n",
    "res = catalog.search(activity_id=activity_ids, source_id=source_id, experiment_id=experiment_ids, \n",
    "                     member_id=member_id, table_id=table_id, variable_id=variable_id)\n",
    "\n",
    "# Display data fram associated with results\n",
    "display(res.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29741927-fedb-4850-b352-6d0bf8454129",
   "metadata": {},
   "source": [
    "Now we're in business! The search above returned five results:\n",
    "\n",
    "- one historical simulation (\"experiment_id\" = \"historical\")\n",
    "- four future projection simulations (\"experiment_id\" = \"ssp585\", \"ssp370\", \"ssp126\", or \"ssp245\")\n",
    "\n",
    "We can now follow a procedure similar to the one we used in previous tutorials, to read in the data as xarray objects and make our time series and map plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad6134-5daf-4151-af5f-c2846115f14f",
   "metadata": {},
   "source": [
    "### **Read information and store as an xarray object**\n",
    "\n",
    "Let's first read in the data for just the historical simulation. To do this, we'll use the following command:\n",
    "\n",
    "`xr.open_zarr(res.df['zstore'][0], storage_options={'anon': True}) `\n",
    "\n",
    "This is part of the `xarray` package, and is designed to allow us to read in information stored as _zarr stores_. \n",
    "\n",
    "_**What's a zarr store??**_\n",
    "\n",
    "`zarr`, like netCDF, is a self-describing data storage format, meaning that all the metadata and coordinate information you need to understand the data is packaged up with the data itself. However, it's been optimized for accessibility via cloud/parallelized servers, which is why many of the cloud-based data catalogs contain data stored in zarr format. _**Essentially, zarr is the cloud-optimized version of a netCDF file!**_\n",
    "\n",
    "The historical data file is the first one returned in our catalog search above. So we want to retrieve the first value in the `zstore` column of the `res` dataframe, which will tell Python how to retrieve the relevant information. \n",
    "\n",
    "The final thing we need to pass to `open_zarr` is a flag that tells it to ignore any login information - since this is a publicly available database, we don't need it. That's what the `storage_options={'anon': True}` argument is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d1b9ca6-5489-4069-bb3e-4292a6cd63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in just the historical data file as a demo\n",
    "hist_data = xr.open_zarr(res.df['zstore'][0], storage_options={'anon': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf685e-9c14-4b15-961d-53207a13cd9d",
   "metadata": {},
   "source": [
    "We can print out the data to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e58a0999-7595-4bf2-854f-65a5d75bb58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 64, bnds: 2, lon: 128, time: 1980)\n",
      "Coordinates:\n",
      "    height     float64 ...\n",
      "  * lat        (lat) float64 -87.86 -85.1 -82.31 -79.53 ... 82.31 85.1 87.86\n",
      "    lat_bnds   (lat, bnds) float64 dask.array<chunksize=(64, 2), meta=np.ndarray>\n",
      "  * lon        (lon) float64 0.0 2.812 5.625 8.438 ... 348.8 351.6 354.4 357.2\n",
      "    lon_bnds   (lon, bnds) float64 dask.array<chunksize=(128, 2), meta=np.ndarray>\n",
      "  * time       (time) object 1850-01-16 12:00:00 ... 2014-12-16 12:00:00\n",
      "    time_bnds  (time, bnds) object dask.array<chunksize=(1980, 2), meta=np.ndarray>\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    tas        (time, lat, lon) float32 dask.array<chunksize=(600, 64, 128), meta=np.ndarray>\n",
      "Attributes: (12/56)\n",
      "    CCCma_model_hash:            55f484f90aff0e32c5a8e92a42c6b9ae7ffe6224\n",
      "    CCCma_parent_runid:          rc3.1-pictrl\n",
      "    CCCma_pycmor_hash:           33c30511acc319a98240633965a04ca99c26427e\n",
      "    CCCma_runid:                 rc3.1-his10\n",
      "    Conventions:                 CF-1.7 CMIP-6.2\n",
      "    YMDH_branch_time_in_child:   1850:01:01:00\n",
      "    ...                          ...\n",
      "    variable_id:                 tas\n",
      "    variant_label:               r10i1p1f1\n",
      "    version:                     v20190429\n",
      "    status:                      2019-10-25;created;by nhn2@columbia.edu\n",
      "    netcdf_tracking_ids:         hdl:21.14100/5ed65f32-a352-4bd1-83a4-c659b4f...\n",
      "    version_id:                  v20190429\n"
     ]
    }
   ],
   "source": [
    "print(hist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85265440-d10a-4627-9dc0-0ed7bc575686",
   "metadata": {},
   "source": [
    "Sure enough, this results in an xarray Dataset that looks essentially identical to the one we got when we downloaded the CanESM5 historical data manually! (compare with the results of the tutorials in the Time Series Plots repo for yourself if you like)\n",
    "\n",
    "Some of the dimensions might be listed in a different order, but that doesn't matter to xarray since it knows how to find them based on their names... now that you've read in the data, you can do anything with it that you would with any other xarray dataset!\n",
    "\n",
    "Now let's read in a second data file, one that goes with one of the SSP future projection simulations: say, SSP3-7.0. Looking at the data table above, we see that this is the third entry - so we grab the location of the third file and feed it to `xr.open_zarr` as we did for the historical simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbfcbeab-7b22-41bc-ac7e-85779450e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for SSP370\n",
    "ssp370_data = xr.open_zarr(res.df['zstore'][2], storage_options={'anon': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847718a-8ea7-4a7e-98c3-f99ac7a402ca",
   "metadata": {},
   "source": [
    "<a id='time_series'></a> \n",
    "## **Plot a Time Series**\n",
    "\n",
    "Now that the data have been read in, we can use it to plot a time series. Let's redo the example from our [first tutorial](http://localhost:8888/lab/tree/Time-Series-Plots/1.%20Read%20in%20Climate%20Data%20%2B%20Plot%20a%20Regionally%20Averaged%20Time%20Series.ipynb): a regionally averaged temperature plot for New York City.\n",
    "\n",
    "First, we'll *also* concatenate the historical and SSP information into a single xarray object, to make the plotting simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f87743c6-66d5-4c9f-8a9a-31d16c602c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate historical and future projection data\n",
    "canesm5_data = xr.concat([hist_data, ssp370_data], dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71913d-dd5e-4a93-b476-634094acbc6e",
   "metadata": {},
   "source": [
    "Recall that the CanESM5 uses a non-standard 365-day year (no leap years), and to get the plotting to work correctly we have to convert the time format to `datetime64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bbcabc2-3769-49f7-b7a8-2e6cbec7a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time to datetime64 format\n",
    "time = canesm5_data.time.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489636de-6f91-44d6-a4cb-b9b08f39395f",
   "metadata": {},
   "source": [
    "Now we follow the rest of the steps from the previous tutorial to define lat/lon bounds, mask the data, and compute a regional average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8685916-91a5-4a08-8d77-66ae80a501d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define min/max bounds for region of interest (NYC)\n",
    "lat_min, lat_max = 40, 41.5\n",
    "lon_min, lon_max = 285.5, 287\n",
    "\n",
    "# Define logical mask: True when lat/lon inside the valid ranges, False elsewhere\n",
    "tas_NYC_lat = (canesm5_data.lat >= lat_min) & (canesm5_data.lat <= lat_max)\n",
    "tas_NYC_lon = (canesm5_data.lon >= lon_min) & (canesm5_data.lon <= lon_max)\n",
    "\n",
    "# Find points where the mask value is True, drop all other points\n",
    "tas_NYC = canesm5_data.where(tas_NYC_lat & tas_NYC_lon, drop=True)\n",
    "\n",
    "# Average over lat, lon dimensions to get a time series\n",
    "tas_NYC = tas_NYC.mean(dim=[\"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61c713-b518-4c44-aeab-8fce0ea045fe",
   "metadata": {},
   "source": [
    "and finally, generate our plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817ec77-0bdc-4bd7-8565-43d4d0c49b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "ax.plot(time, tas_NYC.tas, label='Near-Surface Air Temperature', color='b')\n",
    "ax.set_title(\"Time Series of NYC Near-Surface Air Temperature (1850 to 2100) \", fontsize=20)\n",
    "ax.set_xlabel(\"Time\", fontsize=20)\n",
    "ax.set_ylabel(\"Temperature (K)\", fontsize=20)\n",
    "ax.legend(fontsize=20)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2159a18-f5e3-4a6a-b41d-78ac011f745a",
   "metadata": {},
   "source": [
    "Great job! Now it should be more clear that you can use **EITHER** the manual download **OR** the cloud computing solution to access the same datasets, and do all the same analysis tasks. The ability to quickly pull data down from the cloud makes it much easier to carry out complicated analyses, so it's a great skill to have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99cf6d-09d7-4bb6-9383-16221d1c4439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
